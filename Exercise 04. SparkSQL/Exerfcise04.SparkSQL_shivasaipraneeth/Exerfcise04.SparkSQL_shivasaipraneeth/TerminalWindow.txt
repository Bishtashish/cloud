<<<<< SHIVA SAI PRANEETH CHAKINALA >>>>>
<<<<< STUDENT ID: 801147603 >>>>>



Using username "hadoop".
Authenticating with public key "imported-openssh-key"
Passphrase for key "imported-openssh-key":

       __|  __|_  )
       _|  (     /   Amazon Linux AMI
      ___|\___|___|

https://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/
7 package(s) needed for security, out of 13 available
Run "sudo yum update" to apply all updates.

EEEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR
E::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R
EE:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R
  E::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R
  E::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R
  E:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R
  E::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR
  E:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R
  E::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R
  E::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R
EE:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R
E::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R
EEEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR

[hadoop@ip-172-31-46-235 ~]$ aws s3 cp s3://801147603bucket/SparkSQLScala.jar .
download: s3://801147603bucket/SparkSQLScala.jar to ./SparkSQLScala.jar
[hadoop@ip-172-31-46-235 ~]$ spark-submit --class org.SparkSQL.Driver ./SparkSQLScala.jar s3://801147603bucket/data.txt s3://801147603bucket/SparkSQLOutput
[hadoop@ip-172-31-46-235 ~]$ spark-submit --class org.SparkSQL.Driver ./SparkSQLScala.jar s3://801147603bucket/data.txt s3://801147603bucket/SparkSQLOutput
19/09/23 02:26:42 INFO SparkContext: Running Spark version 2.4.3
19/09/23 02:26:42 INFO SparkContext: Submitted application: SparkAction
19/09/23 02:26:42 INFO SecurityManager: Changing view acls to: hadoop
19/09/23 02:26:42 INFO SecurityManager: Changing modify acls to: hadoop
19/09/23 02:26:42 INFO SecurityManager: Changing view acls groups to:
19/09/23 02:26:42 INFO SecurityManager: Changing modify acls groups to:
19/09/23 02:26:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(hadoop); groups with view permissions: Set(); users  with modify permissions: Set(hadoop); groups with modify permissions: Set()
19/09/23 02:26:42 INFO Utils: Successfully started service 'sparkDriver' on port 43077.
19/09/23 02:26:42 INFO SparkEnv: Registering MapOutputTracker
19/09/23 02:26:42 INFO SparkEnv: Registering BlockManagerMaster
19/09/23 02:26:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/09/23 02:26:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/09/23 02:26:42 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-f795a090-5856-4146-9429-4eea775acb9f
19/09/23 02:26:42 INFO MemoryStore: MemoryStore started with capacity 414.4 MB
19/09/23 02:26:42 INFO SparkEnv: Registering OutputCommitCoordinator
19/09/23 02:26:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/09/23 02:26:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-46-235.us-east-2.compute.internal:4040
19/09/23 02:26:43 INFO SparkContext: Added JAR file:/home/hadoop/./SparkSQLScala.jar at spark://ip-172-31-46-235.us-east-2.compute.internal:43077/jars/SparkSQLScala.jar with timestamp 1569205603157
19/09/23 02:26:43 INFO Executor: Starting executor ID driver on host localhost
19/09/23 02:26:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39795.
19/09/23 02:26:43 INFO NettyBlockTransferService: Server created on ip-172-31-46-235.us-east-2.compute.internal:39795
19/09/23 02:26:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/09/23 02:26:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-46-235.us-east-2.compute.internal, 39795, None)
19/09/23 02:26:43 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-46-235.us-east-2.compute.internal:39795 with 414.4 MB RAM, BlockManagerId(driver, ip-172-31-46-235.us-east-2.compute.internal, 39795, None)
19/09/23 02:26:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-46-235.us-east-2.compute.internal, 39795, None)
19/09/23 02:26:43 INFO BlockManager: external shuffle service port = 7337
19/09/23 02:26:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-46-235.us-east-2.compute.internal, 39795, None)
19/09/23 02:26:44 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/local-1569205603208
19/09/23 02:26:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 237.8 KB, free 414.2 MB)
19/09/23 02:26:45 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 24.1 KB, free 414.2 MB)
19/09/23 02:26:45 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-46-235.us-east-2.compute.internal:39795 (size: 24.1 KB, free: 414.4 MB)
19/09/23 02:26:45 INFO SparkContext: Created broadcast 0 from textFile at Driver.scala:19
19/09/23 02:26:46 INFO SharedState: loading hive config file: file:/etc/spark/conf.dist/hive-site.xml
19/09/23 02:26:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('hdfs:///user/spark/warehouse').
19/09/23 02:26:46 INFO SharedState: Warehouse path is 'hdfs:///user/spark/warehouse'.
19/09/23 02:26:47 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/09/23 02:26:48 INFO CodeGenerator: Code generated in 240.90504 ms
19/09/23 02:26:50 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
19/09/23 02:26:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
19/09/23 02:26:50 INFO DirectFileOutputCommitter: Nothing to setup since the outputs are written directly.
19/09/23 02:26:50 INFO GPLNativeCodeLoader: Loaded native gpl library
19/09/23 02:26:50 INFO LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 7e6c862e89bc8db32c064454a55af74ddff73bae]
19/09/23 02:26:50 INFO FileInputFormat: Total input files to process : 1
19/09/23 02:26:50 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78
19/09/23 02:26:50 INFO DAGScheduler: Got job 0 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions
19/09/23 02:26:50 INFO DAGScheduler: Final stage: ResultStage 0 (runJob at SparkHadoopWriter.scala:78)
19/09/23 02:26:50 INFO DAGScheduler: Parents of final stage: List()
19/09/23 02:26:50 INFO DAGScheduler: Missing parents: List()
19/09/23 02:26:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at saveAsTextFile at Driver.scala:31), which has no missing parents
19/09/23 02:26:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 95.5 KB, free 414.1 MB)
19/09/23 02:26:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 34.7 KB, free 414.1 MB)
19/09/23 02:26:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-172-31-46-235.us-east-2.compute.internal:39795 (size: 34.7 KB, free: 414.4 MB)
19/09/23 02:26:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1201
19/09/23 02:26:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at saveAsTextFile at Driver.scala:31) (first 15 tasks are for partitions Vector(0))
19/09/23 02:26:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
19/09/23 02:26:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 7889 bytes)
19/09/23 02:26:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
19/09/23 02:26:51 INFO Executor: Fetching spark://ip-172-31-46-235.us-east-2.compute.internal:43077/jars/SparkSQLScala.jar with timestamp 1569205603157
19/09/23 02:26:51 INFO TransportClientFactory: Successfully created connection to ip-172-31-46-235.us-east-2.compute.internal/172.31.46.235:43077 after 33 ms (0 ms spent in bootstraps)
19/09/23 02:26:51 INFO Utils: Fetching spark://ip-172-31-46-235.us-east-2.compute.internal:43077/jars/SparkSQLScala.jar to /mnt/tmp/spark-ad4f5a25-7e0b-4984-8099-e9fa47f5f973/userFiles-e2b84a4f-6d38-40d6-9831-a9a17cd329e5/fetchFileTemp9174531023282207934.tmp
19/09/23 02:26:51 INFO Executor: Adding file:/mnt/tmp/spark-ad4f5a25-7e0b-4984-8099-e9fa47f5f973/userFiles-e2b84a4f-6d38-40d6-9831-a9a17cd329e5/SparkSQLScala.jar to class loader
19/09/23 02:26:51 INFO HadoopRDD: Input split: s3://801147603bucket/data.txt:0+53593
19/09/23 02:26:51 INFO S3NativeFileSystem: Opening 's3://801147603bucket/data.txt' for reading
19/09/23 02:26:51 INFO CodeGenerator: Code generated in 40.765001 ms
19/09/23 02:26:51 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.DirectFileOutputCommitter
19/09/23 02:26:51 INFO MultipartUploadOutputStream: close closed:false s3://801147603bucket/SparkSQLOutput/part-00000
19/09/23 02:26:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20190923022649_0008_m_000000_0
19/09/23 02:26:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1619 bytes result sent to driver
19/09/23 02:26:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 583 ms on localhost (executor driver) (1/1)
19/09/23 02:26:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
19/09/23 02:26:51 INFO DAGScheduler: ResultStage 0 (runJob at SparkHadoopWriter.scala:78) finished in 0.746 s
19/09/23 02:26:51 INFO DAGScheduler: Job 0 finished: runJob at SparkHadoopWriter.scala:78, took 0.839755 s
19/09/23 02:26:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 2
19/09/23 02:26:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: true
19/09/23 02:26:51 INFO DirectFileOutputCommitter: Direct Write: ENABLED
19/09/23 02:26:51 INFO DirectFileOutputCommitter: Nothing to clean up since no temporary files were written.
19/09/23 02:26:51 INFO MultipartUploadOutputStream: close closed:false s3://801147603bucket/SparkSQLOutput/_SUCCESS
19/09/23 02:26:51 INFO SparkHadoopWriter: Job job_20190923022649_0008 committed.
19/09/23 02:26:51 INFO SparkContext: Invoking stop() from shutdown hook
19/09/23 02:26:51 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-46-235.us-east-2.compute.internal:4040
19/09/23 02:26:51 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/09/23 02:26:51 INFO MemoryStore: MemoryStore cleared
19/09/23 02:26:51 INFO BlockManager: BlockManager stopped
19/09/23 02:26:51 INFO BlockManagerMaster: BlockManagerMaster stopped
19/09/23 02:26:51 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/09/23 02:26:51 INFO SparkContext: Successfully stopped SparkContext
19/09/23 02:26:51 INFO ShutdownHookManager: Shutdown hook called
19/09/23 02:26:51 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-eb95b953-e9e2-49cc-a724-7a5c0367c169
19/09/23 02:26:51 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-ad4f5a25-7e0b-4984-8099-e9fa47f5f973
[hadoop@ip-172-31-46-235 ~]$
